{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33239bf-f6b7-4f58-b271-dd3440516dc0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/06 13:08:30 INFO SecurityManager: Changing view acls to: olivyatan,b_perso,*\n",
      "24/11/06 13:08:30 INFO SecurityManager: Changing modify acls to: olivyatan,b_perso\n",
      "24/11/06 13:08:30 INFO SecurityManager: Changing view acls groups to: \n",
      "24/11/06 13:08:30 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/11/06 13:08:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(olivyatan, b_perso, *); groups with view permissions: Set(); users  with modify permissions: Set(olivyatan, b_perso); groups with modify permissions: Set()\n",
      "24/11/06 13:08:30 INFO deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "24/11/06 13:08:30 INFO SparkContext: Running Spark version 3.1.1.0.1.0\n",
      "24/11/06 13:08:30 INFO ResourceUtils: ==============================================================\n",
      "24/11/06 13:08:30 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/11/06 13:08:30 INFO ResourceUtils: ==============================================================\n",
      "24/11/06 13:08:30 INFO SparkContext: Submitted application: purchase-suppression-tmp-ol\n",
      "24/11/06 13:08:30 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 4096, script: , vendor: , cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 20480, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/11/06 13:08:30 INFO ResourceProfile: Limiting resource is cpus at 3 tasks per executor\n",
      "24/11/06 13:08:30 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/11/06 13:08:30 INFO SecurityManager: Changing view acls to: olivyatan,b_perso,*\n",
      "24/11/06 13:08:30 INFO SecurityManager: Changing modify acls to: olivyatan,b_perso\n",
      "24/11/06 13:08:30 INFO SecurityManager: Changing view acls groups to: \n",
      "24/11/06 13:08:30 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/11/06 13:08:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(olivyatan, b_perso, *); groups with view permissions: Set(); users  with modify permissions: Set(olivyatan, b_perso); groups with modify permissions: Set()\n",
      "24/11/06 13:08:31 WARN Utils: Service 'sparkDriver' could not bind on port 30202. Attempting port 30203.\n",
      "24/11/06 13:08:31 WARN Utils: Service 'sparkDriver' could not bind on port 30203. Attempting port 30204.\n",
      "24/11/06 13:08:31 WARN Utils: Service 'sparkDriver' could not bind on port 30204. Attempting port 30205.\n",
      "24/11/06 13:08:31 INFO Utils: Successfully started service 'sparkDriver' on port 30205.\n",
      "24/11/06 13:08:31 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/11/06 13:08:31 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/11/06 13:08:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/11/06 13:08:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/11/06 13:08:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/11/06 13:08:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-337e9e9e-dc9b-43af-abc7-66e3fcbab039\n",
      "24/11/06 13:08:31 INFO MemoryStore: MemoryStore started with capacity 10.5 GiB\n",
      "24/11/06 13:08:31 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/11/06 13:08:31 INFO log: Logging initialized @3641ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "24/11/06 13:08:31 INFO Server: jetty-9.4.36.v20210114; built: 2021-01-14T16:44:28.689Z; git: 238ec6997c7806b055319a6d11f8ae7564adc0de; jvm 1.8.0_292-b10\n",
      "24/11/06 13:08:31 INFO Server: Started @3760ms\n",
      "24/11/06 13:08:31 WARN Utils: Service 'SparkUI' could not bind on port 30401. Attempting port 30402.\n",
      "24/11/06 13:08:31 WARN Utils: Service 'SparkUI' could not bind on port 30402. Attempting port 30403.\n",
      "24/11/06 13:08:31 WARN Utils: Service 'SparkUI' could not bind on port 30403. Attempting port 30404.\n",
      "24/11/06 13:08:31 INFO AbstractConnector: Started ServerConnector@39f46c7c{HTTP/1.1, (http/1.1)}{0.0.0.0:30404}\n",
      "24/11/06 13:08:31 INFO Utils: Successfully started service 'SparkUI' on port 30404.\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@335e6853{/jobs,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5e6db233{/jobs/json,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1b776aaf{/jobs/job,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@393de516{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2801557b{/stages,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2f1c79c{/stages/json,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@47b5e5bd{/stages/stage,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@685d6a03{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6bcdee4f{/stages/pool,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7fc83a0{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@538d824f{/storage,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@230c66d7{/storage/json,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7e7303e8{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@ed02cbb{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3220eeee{/environment,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@252374b8{/environment/json,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@12b7829d{/executors,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4ff0a0da{/executors/json,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6cdd22e9{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@137b69eb{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@53bbd8ce{/static,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3abe27bc{/,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4fff3169{/api,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@46909551{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@cf87bcd{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:31 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.198.242.32:30404\n",
      "24/11/06 13:08:31 INFO SparkContext: Added file /data/shpx/notebooks/hroitman/Purchase-Suppression/feature_helpers.py at spark://10.198.242.32:30205/files/feature_helpers.py with timestamp 1730898510627\n",
      "24/11/06 13:08:31 INFO Utils: Copying /data/shpx/notebooks/hroitman/Purchase-Suppression/feature_helpers.py to /tmp/spark-7f198a30-346e-4ea1-a653-e7550a08b52b/userFiles-ad7f5c13-e719-470c-8f5f-e65c851ed32a/feature_helpers.py\n",
      "24/11/06 13:08:32 INFO HadoopDelegationTokenManager: Attempting to load user's ticket cache.\n",
      "24/11/06 13:08:32 INFO HadoopFSDelegationTokenProvider: getting token for: org.apache.hadoop.fs.viewfs.ViewFileSystem@3675af12 with renewer rm/apollo-rno-rm-1.vip.hadoop.ebay.com@PROD.EBAY.COM\n",
      "24/11/06 13:08:33 INFO DFSClient: Created token for b_perso: HDFS_DELEGATION_TOKEN owner=b_perso@PROD.EBAY.COM, renewer=yarn, realUser=, issueDate=1730898513293, maxDate=1731503313293, sequenceNumber=255572227, masterKeyId=34941 on ha-hdfs:apollo-router\n",
      "24/11/06 13:08:33 INFO HadoopFSDelegationTokenProvider: getting token for: org.apache.hadoop.fs.viewfs.ViewFileSystem@3675af12 with renewer b_perso@PROD.EBAY.COM\n",
      "24/11/06 13:08:33 INFO DFSClient: Created token for b_perso: HDFS_DELEGATION_TOKEN owner=b_perso@PROD.EBAY.COM, renewer=b_perso, realUser=, issueDate=1730898513337, maxDate=1731503313337, sequenceNumber=255572228, masterKeyId=34941 on ha-hdfs:apollo-router\n",
      "24/11/06 13:08:33 INFO HadoopFSDelegationTokenProvider: Renewal interval is 86400080 for token HDFS_DELEGATION_TOKEN\n",
      "24/11/06 13:08:34 INFO HiveConf: Found configuration file file:/apache/releases/apache-hive-1.2.1000.2.6.4.2.0.23-bin/conf/hive-site.xml\n",
      "24/11/06 13:08:34 WARN HiveConf: DEPRECATED: hive.metastore.ds.retry.* no longer has any effect.  Use hive.hmshandler.retry.* instead\n",
      "24/11/06 13:08:34 WARN HiveConf: HiveConf of name hive.metastore.local does not exist\n",
      "24/11/06 13:08:34 WARN HiveConf: HiveConf of name hive.enforce.sorting does not exist\n",
      "24/11/06 13:08:34 WARN HiveConf: HiveConf of name hive.server2.proxyuser.hue.groups does not exist\n",
      "24/11/06 13:08:34 WARN HiveConf: HiveConf of name hive.server2.proxyuser.hue.hosts does not exist\n",
      "24/11/06 13:08:34 WARN HiveConf: HiveConf of name hive.metastore.ds.retry.interval does not exist\n",
      "24/11/06 13:08:34 WARN HiveConf: HiveConf of name hive.enforce.bucketing does not exist\n",
      "24/11/06 13:08:34 WARN HiveConf: HiveConf of name hive.metastore.ds.retry.attempts does not exist\n",
      "24/11/06 13:08:34 WARN HiveConf: HiveConf of name hive.server2.enable.impersonation does not exist\n",
      "24/11/06 13:08:34 INFO deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "24/11/06 13:08:34 INFO SparkHadoopUtil: Updating delegation tokens for current user.\n",
      "24/11/06 13:08:34 INFO Utils: Using initial executors = 10, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "24/11/06 13:08:35 INFO RemoteConfig: Merged with remote configs\n",
      "24/11/06 13:08:35 INFO Client: Requesting a new application from cluster with 12678 NodeManagers\n",
      "24/11/06 13:08:35 INFO Configuration: resource-types.xml not found\n",
      "24/11/06 13:08:35 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "24/11/06 13:08:36 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (65536 MB per container)\n",
      "24/11/06 13:08:36 INFO Client: Will allocate AM container, with 4505 MB memory including 409 MB overhead\n",
      "24/11/06 13:08:36 INFO Client: Setting up container launch context for our AM\n",
      "24/11/06 13:08:36 INFO Client: Setting up the launch environment for our AM container\n",
      "24/11/06 13:08:36 INFO Client: Preparing resources for our AM container\n",
      "24/11/06 13:08:36 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/11/06 13:08:37 INFO Client: Uploading resource file:/tmp/spark-7f198a30-346e-4ea1-a653-e7550a08b52b/__spark_libs__7562495628336815005.zip -> viewfs://apollo-rno/user/b_perso/.sparkStaging/application_1728324362732_7485232/__spark_libs__7562495628336815005.zip\n",
      "24/11/06 13:08:38 INFO Client: Uploading resource file:/data/shpx/notebooks/hroitman/Purchase-Suppression/feature_helpers.py -> viewfs://apollo-rno/user/b_perso/.sparkStaging/application_1728324362732_7485232/feature_helpers.py\n",
      "24/11/06 13:08:39 INFO Client: Uploading resource file:/data/shpx/data/mpalei/spark-3.1.1.0.1.0-bin-ebay/python/lib/pyspark.zip -> viewfs://apollo-rno/user/b_perso/.sparkStaging/application_1728324362732_7485232/pyspark.zip\n",
      "24/11/06 13:08:39 INFO Client: Uploading resource file:/data/shpx/data/mpalei/spark-3.1.1.0.1.0-bin-ebay/python/lib/py4j-0.10.9-src.zip -> viewfs://apollo-rno/user/b_perso/.sparkStaging/application_1728324362732_7485232/py4j-0.10.9-src.zip\n",
      "24/11/06 13:08:39 INFO Client: Uploading resource file:/tmp/spark-7f198a30-346e-4ea1-a653-e7550a08b52b/__spark_conf__1527057979551503113.zip -> viewfs://apollo-rno/user/b_perso/.sparkStaging/application_1728324362732_7485232/__spark_conf__.zip\n",
      "24/11/06 13:08:39 INFO SecurityManager: Changing view acls to: olivyatan,b_perso,*\n",
      "24/11/06 13:08:39 INFO SecurityManager: Changing modify acls to: olivyatan,b_perso\n",
      "24/11/06 13:08:39 INFO SecurityManager: Changing view acls groups to: \n",
      "24/11/06 13:08:39 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/11/06 13:08:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(olivyatan, b_perso, *); groups with view permissions: Set(); users  with modify permissions: Set(olivyatan, b_perso); groups with modify permissions: Set()\n",
      "24/11/06 13:08:39 INFO Client: Submitting application application_1728324362732_7485232 to ResourceManager\n",
      "24/11/06 13:08:41 INFO YarnClientImpl: Application submission is not finished, submitted application application_1728324362732_7485232 is still in SUBMITTED\n",
      "24/11/06 13:08:43 INFO YarnClientImpl: Application submission is not finished, submitted application application_1728324362732_7485232 is still in SUBMITTED\n",
      "24/11/06 13:08:44 INFO YarnClientImpl: Submitted application application_1728324362732_7485232\n",
      "24/11/06 13:08:45 INFO Client: Application report for application_1728324362732_7485232 (state: ACCEPTED)\n",
      "24/11/06 13:08:45 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: hddq-exprce-perso-high-mem\n",
      "\t start time: 1730898519394\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: https://apollo-rno-rm-1.vip.hadoop.ebay.com:50030/proxy/application_1728324362732_7485232/\n",
      "\t user: b_perso\n",
      "24/11/06 13:08:46 INFO Client: Application report for application_1728324362732_7485232 (state: ACCEPTED)\n",
      "24/11/06 13:08:47 INFO Client: Application report for application_1728324362732_7485232 (state: ACCEPTED)\n",
      "24/11/06 13:08:48 INFO Client: Application report for application_1728324362732_7485232 (state: ACCEPTED)\n",
      "24/11/06 13:08:49 INFO Client: Application report for application_1728324362732_7485232 (state: ACCEPTED)\n",
      "24/11/06 13:08:50 INFO Client: Application report for application_1728324362732_7485232 (state: ACCEPTED)\n",
      "24/11/06 13:08:51 INFO Client: Application report for application_1728324362732_7485232 (state: ACCEPTED)\n",
      "24/11/06 13:08:52 INFO Client: Application report for application_1728324362732_7485232 (state: ACCEPTED)\n",
      "24/11/06 13:08:53 INFO Client: Application report for application_1728324362732_7485232 (state: ACCEPTED)\n",
      "24/11/06 13:08:54 INFO Client: Application report for application_1728324362732_7485232 (state: ACCEPTED)\n",
      "24/11/06 13:08:55 INFO Client: Application report for application_1728324362732_7485232 (state: ACCEPTED)\n",
      "24/11/06 13:08:56 INFO Client: Application report for application_1728324362732_7485232 (state: ACCEPTED)\n",
      "24/11/06 13:08:57 INFO Client: Application report for application_1728324362732_7485232 (state: ACCEPTED)\n",
      "24/11/06 13:08:58 INFO Client: Application report for application_1728324362732_7485232 (state: ACCEPTED)\n",
      "24/11/06 13:08:59 INFO Client: Application report for application_1728324362732_7485232 (state: RUNNING)\n",
      "24/11/06 13:08:59 INFO Client: \n",
      "\t client token: Token { kind: YARN_CLIENT_TOKEN, service:  }\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 10.179.18.35\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: hddq-exprce-perso-high-mem\n",
      "\t start time: 1730898519394\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: https://apollo-rno-rm-1.vip.hadoop.ebay.com:50030/proxy/application_1728324362732_7485232/\n",
      "\t user: b_perso\n",
      "24/11/06 13:08:59 INFO YarnClientSchedulerBackend: Application application_1728324362732_7485232 has started running.\n",
      "24/11/06 13:08:59 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 30601. Attempting port 30602.\n",
      "24/11/06 13:08:59 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 30602. Attempting port 30603.\n",
      "24/11/06 13:08:59 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 30603. Attempting port 30604.\n",
      "24/11/06 13:08:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 30604.\n",
      "24/11/06 13:08:59 INFO NettyBlockTransferService: Server created on 10.198.242.32:30604\n",
      "24/11/06 13:08:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/11/06 13:08:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.198.242.32, 30604, None)\n",
      "24/11/06 13:08:59 INFO BlockManagerMasterEndpoint: Registering block manager 10.198.242.32:30604 with 10.5 GiB RAM, BlockManagerId(driver, 10.198.242.32, 30604, None)\n",
      "24/11/06 13:08:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.198.242.32, 30604, None)\n",
      "24/11/06 13:08:59 INFO BlockManager: external shuffle service port = 7337\n",
      "24/11/06 13:08:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.198.242.32, 30604, None)\n",
      "24/11/06 13:08:59 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2647337b{/metrics/json,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:08:59 INFO SingleEventLogFileWriter: Logging events to hdfs://apollo-rno/spark-logs/application_1728324362732_7485232.lz4.inprogress\n",
      "24/11/06 13:08:59 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> apollo-rno-rm-1.vip.hadoop.ebay.com,apollo-rno-rm-2.vip.hadoop.ebay.com, PROXY_URI_BASES -> https://apollo-rno-rm-1.vip.hadoop.ebay.com:50030/proxy/application_1728324362732_7485232,https://apollo-rno-rm-2.vip.hadoop.ebay.com:50030/proxy/application_1728324362732_7485232, RM_HA_URLS -> apollo-rno-rm-1.vip.hadoop.ebay.com:50030,apollo-rno-rm-2.vip.hadoop.ebay.com:50030), /proxy/application_1728324362732_7485232\n",
      "24/11/06 13:08:59 INFO Utils: Using initial executors = 10, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "24/11/06 13:08:59 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n",
      "24/11/06 13:09:01 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "24/11/06 13:09:02 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\n",
      "24/11/06 13:09:02 INFO SharedState: Setting hive.metastore.warehouse.dir ('/user/hive/warehouse') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse').\n",
      "24/11/06 13:09:02 INFO SharedState: Warehouse path is '/user/hive/warehouse'.\n",
      "24/11/06 13:09:02 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "24/11/06 13:09:02 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5a5bee7b{/SQL,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:09:02 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "24/11/06 13:09:02 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6c0b49be{/SQL/json,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:09:02 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "24/11/06 13:09:02 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3849e3af{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:09:02 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "24/11/06 13:09:02 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@57c5d87d{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "24/11/06 13:09:02 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "24/11/06 13:09:02 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@509f894d{/static/sql,null,AVAILABLE,@Spark}\n"
     ]
    }
   ],
   "source": [
    "from utils import spark_session, save_table\n",
    "\n",
    "session = spark_session(\"purchase-suppression-tmp-ol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dea7a6-ece0-4fea-aa0b-6136f3326d00",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6826e74a-a19c-44ba-8c2e-75a96b8ffd32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = dict()\n",
    "config[\"filters\"] = {\n",
    "    \"start_dt\": \"2023-07-01\",\n",
    "    \"end_dt\": \"2024-07-01\",\n",
    "    \"site_ids\": [0],\n",
    "}\n",
    "\n",
    "config['event_limits'] = {\n",
    "    'max_purchases': 1000,\n",
    "    'max_vi': 1000,\n",
    "    'max_searches': 1000,\n",
    "    'max_watches': 1000,\n",
    "    'max_events': 1000  \n",
    "}\n",
    "\n",
    "config['sampling'] = {\n",
    "    'max_samples': 100000,\n",
    "    'sample_ratio': 0.1,\n",
    "    'ref_dt': '2024-05-01'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078d4422-c719-4bbc-a1f0-cef79f89d01d",
   "metadata": {},
   "source": [
    "# Sample Datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc7bef5-470d-4d60-95c9-80e80ca7972b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from user_sample import sample_data\n",
    "\n",
    "df_sample = sample_data(session, config)\n",
    "save_table(session, df_sample, \"bx_purchase_suppression_samples_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6472a478-ab26-4aa4-a0fb-3a3994f6fa7a",
   "metadata": {},
   "source": [
    "# Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7547a7c5-f821-4cd8-9b77-26f6b425e5f6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/01 07:57:31 ERROR AsyncEventQueue: Listener EventLoggingListener threw an exception\n",
      "java.util.ConcurrentModificationException\n",
      "\tat java.util.Hashtable$Enumerator.next(Hashtable.java:1387)\n",
      "\tat scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$6.next(Wrappers.scala:424)\n",
      "\tat scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$6.next(Wrappers.scala:420)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.mutable.MapLike.toSeq(MapLike.scala:75)\n",
      "\tat scala.collection.mutable.MapLike.toSeq$(MapLike.scala:72)\n",
      "\tat scala.collection.mutable.AbstractMap.toSeq(Map.scala:82)\n",
      "\tat org.apache.spark.scheduler.EventLoggingListener.redactProperties(EventLoggingListener.scala:290)\n",
      "\tat org.apache.spark.scheduler.EventLoggingListener.onJobStart(EventLoggingListener.scala:162)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "IOPub message rate exceeded.) / 1453654][Stage 80:>        (1314 + 0) / 1453654]4]\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.) / 1453654][Stage 83:>              (0 + 0) / 1000]\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.) / 1453654][Stage 83:>              (0 + 0) / 1000]\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.) / 1453654][Stage 83:>              (0 + 0) / 1000]\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "[Stage 85:===============================================> (9696 + 300) / 10000]\r"
     ]
    }
   ],
   "source": [
    "from pyspark_query import get_minimal_vi_histories, get_minimal_purchase_histories, get_leaf_cat_data\n",
    "\n",
    "df = session.table(\"bx_purchase_suppression_samples_1\")\n",
    "df_users = df.select(\"BUYER_ID\").dropDuplicates().cache()\n",
    "\n",
    "# #for data point obtain the item's metadata\n",
    "df_dpoints = get_leaf_cat_data(session, config, df)\n",
    "save_table(session, df_dpoints, \"bx_ps_samples_enriched_1\")#df_smp\n",
    "\n",
    "#for each user obtain purchase history\n",
    "df_prch_hist = get_minimal_purchase_histories(session, config, df_users)\n",
    "save_table(session, df_prch_hist, \"bx_ps_user_prch_hist_1\")#df_prch\n",
    "\n",
    "#for each user obtain vi history\n",
    "df_vi_hist = get_minimal_vi_histories(session, config, df_users)\n",
    "save_table(session, df_vi_hist, \"bx_ps_user_vi_hist_1\")#df_vi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fb44d3-9e55-432a-a7f4-0ce982fc6c95",
   "metadata": {},
   "source": [
    "#### Add watches , searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40488f55-dd14-4858-b3b3-c8651700aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import features as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07c803e0-d564-4b4b-b67c-7b96d40c5b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = session.table(\"bx_purchase_suppression_samples_1\")\n",
    "df_users = df.select(\"BUYER_ID\").dropDuplicates().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2fb2e41-7642-4024-b912-364d7178c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark_query import get_minimal_watch_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "511bbb9a-a9e1-41d3-ad73-bf801ecb05d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wtch_hist = get_minimal_watch_histories(session, config, df_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "575041f0-d82d-4c41-968c-ba31d0bbd441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13707413"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wtch_hist.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faaf21d7-c072-4c62-8159-fd49eb77a24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#save_table(session, df_wtch_hist, \"bx_ps_user_watch_hist_1\")#df_watch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6dd2dd9a-0a57-4376-9cda-b7d9875c97e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import functions as F\n",
    "#from pyspark.sql.functions import udf, first, lit, split, explode\n",
    "#import pyspark_query as pq\n",
    "\n",
    "def get_minimal_watch_histories(session, config, df_users_to_consider):\n",
    "    # Get watch histories from bbowac_event_fact with event_type_desc='Watch'\n",
    "    df_user_histories = session.table(\"access_views.bbowac_event_fact\") \\\n",
    "        .where((F.col(\"session_start_dt\") >= config['filters']['start_dt']) &  \n",
    "               (F.col(\"session_start_dt\") <= config['filters']['end_dt']) &\n",
    "               (F.upper(F.col(\"site_id\")).isin(config['filters']['site_ids'])) &  \n",
    "               (F.col(\"event_type_desc\") == 'Watch') &  # Filter where event_type_desc is 'Watch'\n",
    "               (F.col(\"bot_session\") == 0))  # Exclude bot sessions\n",
    "\n",
    "    # Rename columns\n",
    "    df_user_histories = df_user_histories \\\n",
    "        .withColumnRenamed('buyer_id', 'BUYER_ID') \\\n",
    "        .withColumnRenamed('site_id', 'SITE_ID') \\\n",
    "        .withColumnRenamed('item_id', 'ITEM_ID') \\\n",
    "        .withColumnRenamed('variation_id', 'VARIATION_ID') \\\n",
    "        .withColumnRenamed('leaf_categ_id', 'LEAF_CATEG_ID')  \\\n",
    "        .withColumnRenamed('event_timestamp', 'EVENT_TIMESTAMP')  \\\n",
    "        .select('EVENT_TIMESTAMP', 'BUYER_ID', 'ITEM_ID', 'VARIATION_ID', 'LEAF_CATEG_ID', 'SITE_ID') \n",
    "    \n",
    "    df_user_histories = df_user_histories.join(df_users_to_consider, \"BUYER_ID\") \\\n",
    "        .select('EVENT_TIMESTAMP', 'BUYER_ID', 'ITEM_ID')\n",
    "\n",
    "    # Limit history length using max_watches\n",
    "    df_user_histories = pq.limit_history_len(config['event_limits']['max_watches'],  \n",
    "                                          config['event_limits']['max_events'],\n",
    "                                          df_user_histories) \n",
    "    \n",
    "    df_item_ids = df_user_histories.select(\"ITEM_ID\").dropDuplicates() \n",
    "    \n",
    "    df_item_fact = session.table(\"access_views.DW_LSTG_ITEM\") \\\n",
    "        .select(\"AUCT_TITL\", \"ITEM_ID\", \"AUCT_END_DT\", \"LEAF_CATEG_ID\", \"ITEM_SITE_ID\") \\\n",
    "        .withColumnRenamed(\"ITEM_SITE_ID\", \"SITE_ID\") \\\n",
    "        .withColumnRenamed(\"AUCT_TITL\", \"TITLE\") \\\n",
    "        .where(F.col('AUCT_END_DT') >= config['filters']['start_dt']) \\\n",
    "        .join(df_item_ids, \"ITEM_ID\") \\\n",
    "        .dropDuplicates([\"ITEM_ID\"])\n",
    "    \n",
    "    df_item_fact = pq.lowercase_string_columns(df_item_fact) \n",
    "   \n",
    "    df_final = df_user_histories \\\n",
    "        .join(df_item_fact.select('ITEM_ID', 'TITLE', 'SITE_ID', \"LEAF_CATEG_ID\"), on='ITEM_ID') \\\n",
    "        .join(pq.get_category_names(session, df_item_fact), on=[\"SITE_ID\", \"LEAF_CATEG_ID\"]) \\\n",
    "        .withColumn('EVENT_TYPE', lit('WATCH')) \n",
    "   \n",
    "    df_final_watch = df_final.filter(F.col('ITEM_ID').isNotNull() & \n",
    "                                  F.col('SITE_ID').isNotNull() &\n",
    "                                  F.col('LEAF_CATEG_ID').isNotNull() & \n",
    "                                  F.col('TITLE').isNotNull() &\n",
    "                                  F.col('LEAF_CATEG_NAME').isNotNull())\n",
    "\n",
    "    return pq.lowercase_string_columns(df_final_watch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3d2228f-d11d-47e7-b7e9-3fd2340c6316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark_query import get_minimal_watch_histories_ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c37833e-eaf3-4c09-8f44-970a456ac569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each user obtain search history\n",
    "#df_wtch_hist = get_minimal_watch_histories_ol(session, config, df_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aedcf731-cd65-4a54-bc5c-d25734333b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each user obtain search history\n",
    "df_wtch_hist = get_minimal_watch_histories(session, config, df_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c69b73b6-46a9-486e-84cc-ffc9e26e3e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]]]]]\r"
     ]
    }
   ],
   "source": [
    "#save_table(session, df_wtch_hist, \"bx_ps_user_wtch_hist_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72b41040-3756-451e-9dd0-462909bffc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1868526459"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wtch_hist.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eabd1d73-0ae4-4b61-83eb-1a688f757f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                1413]]]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(EVENT_TIMESTAMP=datetime.datetime(2023, 12, 3, 3, 54, 9, 201000), BUYER_ID=Decimal('112441611'), LEAF_CATEG_ID=Decimal('25815'), ITEM_ID=Decimal('115963551824'), VARIATION_ID=None, SITE_ID=Decimal('0'), EVENT_TYPE='watch', LEAF_CATEG_NAME='home & garden:bedding:bedding accessories', CATEG_LVL2_ID=Decimal('20444'), META_CATEG_ID=Decimal('11700'), CATEG_LVL2_NAME='bedding', META_CATEG_NAME='home & garden')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wtch_hist.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd0c57-db76-4c55-ab2e-70b2283e9253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b0ee39b-3f89-44af-bb71-f21cca7203c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_srch_hist = session.table(\"bx_ps_user_srch_hist_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20581edb-18b1-4b94-b6b1-d2ff795ccc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vi_hist = session.table(\"bx_ps_user_vi_hist_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e65bb0-4c68-45cd-83bb-801deab459bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wtch_hist.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efa9034e-8830-4e61-a6c8-d1a34dbe24a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "53493087"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vi_hist.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56418124-d326-489a-98ac-321944a11923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33368617"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_srch_hist.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7073d3-1447-417e-91e6-a1a336d1188e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if session:\n",
    "    session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cca904d-1b36-453d-bf7a-0a34c15da0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(EVENT_TIMESTAMP=datetime.datetime(2023, 10, 18, 19, 3, 49, 585000), BUYER_ID=Decimal('183988313'), LEAF_CATEG_ID=25815, TITLE='sheet straps', SITE_ID=Decimal('0'), EVENT_TYPE='search', LEAF_CATEG_NAME='home & garden:bedding:bedding accessories', CATEG_LVL2_ID=Decimal('20444'), META_CATEG_ID=Decimal('11700'), CATEG_LVL2_NAME='bedding', META_CATEG_NAME='home & garden')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_srch_hist.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c72f6a8-7d8b-4e57-943f-90ed9b834f34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
