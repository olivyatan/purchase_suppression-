{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25520c2-33df-47e0-87cb-a3064ff1ced1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import spark_session\n",
    "\n",
    "session = spark_session(\"purchase-suppression-tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee919cb-3230-4d0f-a971-279e2ed1c54f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train/eval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1862fe-ec4c-4cc3-8b47-965cbe239264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = session.table(\"bx_ps_features_train\").toPandas()\n",
    "eval_df = session.table(\"bx_ps_features_valid\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3811dc1f-6e15-42b6-ad7e-ed551d56f1e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "def train_eval_model(hyperp, feature_list, label, train_df, eval_df, verbose=True):\n",
    "    model = xgb.XGBClassifier(  \n",
    "        **hyperp\n",
    "    )\n",
    "    model = model.fit(train_df[feature_list], train_df[label], verbose=verbose)\n",
    "    eval_df['preds'] = model.predict(eval_df[feature_list])\n",
    "    return model, eval_df\n",
    "\n",
    "feature_list = [f for f in train_df.columns if f.startswith(\"f_\")]\n",
    "\n",
    "hyperp = {\n",
    "    'tree_method': 'hist',\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'binary:logistic',\n",
    "    'random_state': 42, \n",
    "    'learning_rate': 0.01,\n",
    "    'colsample_bytree': 0.5, \n",
    "    'eta': 0.05, \n",
    "    'max_depth': 12,\n",
    "    'n_estimators': 500,\n",
    "    'subsample': 0.75,\n",
    "    'lambda': 100\n",
    "}\n",
    "\n",
    "model, eval_df = train_eval_model(hyperp, feature_list, \"label\", train_df, eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c3e8fb-70a4-4948-abae-10fc4e146dc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b51fb-203c-4d8f-a63a-53864742ca71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, classification_report\n",
    "\n",
    "y_test, y_pred = eval_df['label'], eval_df['preds']\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "precision_micro = precision_score(y_test, y_pred, average='micro')\n",
    "recall_micro = recall_score(y_test, y_pred, average='micro')\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "label_counts = eval_df['label'].value_counts()\n",
    "print(\"Label counts:\")\n",
    "print(label_counts)\n",
    "\n",
    "label_counts = eval_df['label_type'].value_counts()\n",
    "print(\"Label type counts:\")\n",
    "print(label_counts)\n",
    "\n",
    "print(\"Metrics:\")\n",
    "print(f'ROC AUC: {roc_auc}')\n",
    "print(f'Precision (micro): {precision_micro}')\n",
    "print(f'Recall (micro): {recall_micro}')\n",
    "print(f'Precision (macro): {precision_macro}')\n",
    "print(f'Recall (macro): {recall_macro}')\n",
    "report = classification_report(y_test, y_pred, target_names=['0', '1'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf1103-3cb9-4346-afd5-ddb8172dad60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Compute ROC curve and ROC area\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426861ce-9998-4a3d-8047-f96c0a232059",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc729b47-3180-44cb-b0fe-f8997dca8b51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature importance\n",
    "import numpy as np\n",
    "print(f\"Feature importance of ranker:\")\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_order = []\n",
    "for f in range(train_df.shape[1]):\n",
    "    if f < len(indices):\n",
    "        print(\n",
    "            \"%2d) %-*s %f\" % (f + 1, 30, feature_list[indices[f]], importances[indices[f]])\n",
    "        )\n",
    "        feature_order.append(feature_list[indices[f]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953c032-ad81-4c8d-be94-b948e2a5f533",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# leakage analysis\n",
    "# correlations to detect features with perfect correlation to labels\n",
    "top_30_features = feature_order[0:30]\n",
    "correlations = train_df[top_30_features + ['label']].corr()\n",
    "label_correlations = correlations['label']\n",
    "\n",
    "print(label_correlations)\n",
    "\n",
    "# ROC analysis - fitting a model on a single feature at a time\n",
    "print(\"\\nROC Analysis\")\n",
    "for i, feature_name in enumerate(top_30_features):\n",
    "    single_feature = [feature_name]\n",
    "    single_feature_model, eval_df = train_eval_model(hyperp, single_feature, \"label\", train_df, eval_df)\n",
    "    print(f\"Feature: {feature_name} ({label_correlations.iloc[i]})\")\n",
    "    y_test, y_pred = eval_df['label'], eval_df['preds']\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    precision_micro = precision_score(y_test, y_pred, average='micro')\n",
    "    recall_micro = recall_score(y_test, y_pred, average='micro')\n",
    "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    label_counts = eval_df['label'].value_counts()\n",
    "    print(\"Label counts:\")\n",
    "    print(label_counts)\n",
    "\n",
    "    label_counts = eval_df['label_type'].value_counts()\n",
    "    print(\"Label type counts:\")\n",
    "    print(label_counts)\n",
    "\n",
    "    print(\"Metrics:\")\n",
    "    print(f'ROC AUC: {roc_auc}')\n",
    "    print(f'Precision (micro): {precision_micro}')\n",
    "    print(f'Recall (micro): {recall_micro}')\n",
    "    print(f'Precision (macro): {precision_macro}')\n",
    "    print(f'Recall (macro): {recall_macro}')\n",
    "    report = classification_report(y_test, y_pred, target_names=['0', '1'])\n",
    "    print(report)\n",
    "    print(\"****************************************************************\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ac4ec9-1355-4113-944f-0b1f9de765af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature distributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top_30_features = feature_order[0:30]\n",
    "for i, feature_name in enumerate(top_30_features):\n",
    "    train_df[feature_name].hist(bins=50)\n",
    "    plt.title(feature_name)\n",
    "    plt.xlabel('Feature Values')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc014ec7-fac6-4ab9-abba-0e648b427bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if session:\n",
    "    session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5635e4da-8f0e-4247-b838-4db1e4b0df2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
